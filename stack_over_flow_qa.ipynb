{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stack_over_flow_qa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrezende/stack_over_flow_qa/blob/master/stack_over_flow_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rHZl1wq-Zs2w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras models for question answering problems from "
      ]
    },
    {
      "metadata": {
        "id": "xCkMokFMZ0mk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from abc import abstractmethod\n",
        "\n",
        "from keras.engine import Input\n",
        "from keras.layers import merge, Embedding, Dropout, Conv1D, Lambda, LSTM, Dense, concatenate, TimeDistributed\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LanguageModel:\n",
        "    def __init__(self, config):\n",
        "        self.question = Input(shape=(config['question_len'],), dtype='int32', name='question_base')\n",
        "        self.answer_good = Input(shape=(config['answer_len'],), dtype='int32', name='answer_good_base')\n",
        "        self.answer_bad = Input(shape=(config['answer_len'],), dtype='int32', name='answer_bad_base')\n",
        "\n",
        "        self.config = config\n",
        "        self.params = config.get('similarity', dict())\n",
        "\n",
        "        # initialize a bunch of variables that will be set later\n",
        "        self._models = None\n",
        "        self._similarities = None\n",
        "        self._answer = None\n",
        "        self._qa_model = None\n",
        "\n",
        "        self.training_model = None\n",
        "        self.prediction_model = None\n",
        "\n",
        "    def get_answer(self):\n",
        "        if self._answer is None:\n",
        "            self._answer = Input(shape=(self.config['answer_len'],), dtype='int32', name='answer')\n",
        "        return self._answer\n",
        "\n",
        "    @abstractmethod\n",
        "    def build(self):\n",
        "        return\n",
        "\n",
        "    def get_similarity(self):\n",
        "        ''' Specify similarity in configuration under 'similarity' -> 'mode'\n",
        "        If a parameter is needed for the model, specify it in 'similarity'\n",
        "\n",
        "        Example configuration:\n",
        "\n",
        "        config = {\n",
        "            ... other parameters ...\n",
        "            'similarity': {\n",
        "                'mode': 'gesd',\n",
        "                'gamma': 1,\n",
        "                'c': 1,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        cosine: dot(a, b) / sqrt(dot(a, a) * dot(b, b))\n",
        "        polynomial: (gamma * dot(a, b) + c) ^ d\n",
        "        sigmoid: tanh(gamma * dot(a, b) + c)\n",
        "        rbf: exp(-gamma * l2_norm(a-b) ^ 2)\n",
        "        euclidean: 1 / (1 + l2_norm(a - b))\n",
        "        exponential: exp(-gamma * l2_norm(a - b))\n",
        "        gesd: euclidean * sigmoid\n",
        "        aesd: (euclidean + sigmoid) / 2\n",
        "        '''\n",
        "\n",
        "        params = self.params\n",
        "        similarity = params['mode']\n",
        "\n",
        "        dot = lambda a, b: K.batch_dot(a, b, axes=1)\n",
        "        l2_norm = lambda a, b: K.sqrt(K.sum(K.square(a - b), axis=1, keepdims=True))\n",
        "\n",
        "        if similarity == 'cosine':\n",
        "            return lambda x: dot(x[0], x[1]) / K.maximum(K.sqrt(dot(x[0], x[0]) * dot(x[1], x[1])), K.epsilon())\n",
        "        elif similarity == 'polynomial':\n",
        "            return lambda x: (params['gamma'] * dot(x[0], x[1]) + params['c']) ** params['d']\n",
        "        elif similarity == 'sigmoid':\n",
        "            return lambda x: K.tanh(params['gamma'] * dot(x[0], x[1]) + params['c'])\n",
        "        elif similarity == 'rbf':\n",
        "            return lambda x: K.exp(-1 * params['gamma'] * l2_norm(x[0], x[1]) ** 2)\n",
        "        elif similarity == 'euclidean':\n",
        "            return lambda x: 1 / (1 + l2_norm(x[0], x[1]))\n",
        "        elif similarity == 'exponential':\n",
        "            return lambda x: K.exp(-1 * params['gamma'] * l2_norm(x[0], x[1]))\n",
        "        elif similarity == 'gesd':\n",
        "            euclidean = lambda x: 1 / (1 + l2_norm(x[0], x[1]))\n",
        "            sigmoid = lambda x: 1 / (1 + K.exp(-1 * params['gamma'] * (dot(x[0], x[1]) + params['c'])))\n",
        "            return lambda x: euclidean(x) * sigmoid(x)\n",
        "        elif similarity == 'aesd':\n",
        "            euclidean = lambda x: 0.5 / (1 + l2_norm(x[0], x[1]))\n",
        "            sigmoid = lambda x: 0.5 / (1 + K.exp(-1 * params['gamma'] * (dot(x[0], x[1]) + params['c'])))\n",
        "            return lambda x: euclidean(x) + sigmoid(x)\n",
        "        else:\n",
        "            raise Exception('Invalid similarity: {}'.format(similarity))\n",
        "\n",
        "    def get_qa_model(self):\n",
        "        if self._models is None:\n",
        "            self._models = self.build()\n",
        "\n",
        "        if self._qa_model is None:\n",
        "            question_output, answer_output = self._models\n",
        "            # dropout = Dropout(self.params.get('dropout', 0.2))\n",
        "            similarity = self.get_similarity()\n",
        "            # qa_model = merge([dropout(question_output), dropout(answer_output)],\n",
        "            #                  mode=similarity, output_shape=lambda _: (None, 1))\n",
        "            # qa_model = Lambda(similarity, output_shape=lambda _: (None, 1))([dropout(question_output),\n",
        "            #                                                                  dropout(answer_output)])\n",
        "            qa_model = Lambda(similarity, output_shape=lambda _: (None, 1))([question_output,\n",
        "                                                                             answer_output])\n",
        "            self._qa_model = Model(inputs=[self.question, self.get_answer()], outputs=qa_model, name='qa_model')\n",
        "\n",
        "        return self._qa_model\n",
        "\n",
        "    def compile(self, optimizer, **kwargs):\n",
        "        qa_model = self.get_qa_model()\n",
        "\n",
        "        good_similarity = qa_model([self.question, self.answer_good])\n",
        "        bad_similarity = qa_model([self.question, self.answer_bad])\n",
        "\n",
        "        # loss = merge([good_similarity, bad_similarity],\n",
        "        #              mode=lambda x: K.relu(self.config['margin'] - x[0] + x[1]),\n",
        "        #              output_shape=lambda x: x[0])\n",
        "\n",
        "        loss = Lambda(lambda x: K.relu(self.config['margin'] - x[0] + x[1]),\n",
        "                      output_shape=lambda x: x[0])([good_similarity, bad_similarity])\n",
        "\n",
        "        self.prediction_model = Model(inputs=[self.question, self.answer_good], outputs=good_similarity,\n",
        "                                      name='prediction_model')\n",
        "        self.prediction_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer=optimizer, **kwargs)\n",
        "\n",
        "        self.training_model = Model(inputs=[self.question, self.answer_good, self.answer_bad], outputs=loss,\n",
        "                                    name='training_model')\n",
        "        self.training_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer=optimizer, **kwargs)\n",
        "\n",
        "    def fit(self, x, **kwargs):\n",
        "        assert self.training_model is not None, 'Must compile the model before fitting data'\n",
        "        y = np.zeros(shape=(x[0].shape[0],)) # doesn't get used\n",
        "        return self.training_model.fit(x, y, **kwargs)\n",
        "\n",
        "    def predict(self, x):\n",
        "        assert self.prediction_model is not None and isinstance(self.prediction_model, Model)\n",
        "        return self.prediction_model.predict_on_batch(x)\n",
        "\n",
        "    def save_weights(self, file_name, **kwargs):\n",
        "        assert self.prediction_model is not None, 'Must compile the model before saving weights'\n",
        "        self.prediction_model.save_weights(file_name, **kwargs)\n",
        "\n",
        "    def load_weights(self, file_name, **kwargs):\n",
        "        assert self.prediction_model is not None, 'Must compile the model loading weights'\n",
        "        self.prediction_model.load_weights(file_name, **kwargs)\n",
        "\n",
        "\n",
        "class EmbeddingModel(LanguageModel):\n",
        "    def build(self):\n",
        "        question = self.question\n",
        "        answer = self.get_answer()\n",
        "\n",
        "        # add embedding layers\n",
        "        weights = np.load(self.config['initial_embed_weights'])\n",
        "        embedding = Embedding(input_dim=self.config['n_words'],\n",
        "                              output_dim=weights.shape[1],\n",
        "                              mask_zero=True,\n",
        "                              # dropout=0.2,\n",
        "                              weights=[weights])\n",
        "        question_embedding = embedding(question)\n",
        "        answer_embedding = embedding(answer)\n",
        "\n",
        "        # maxpooling\n",
        "        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "        maxpool.supports_masking = True\n",
        "        question_pool = maxpool(question_embedding)\n",
        "        answer_pool = maxpool(answer_embedding)\n",
        "\n",
        "        return question_pool, answer_pool\n",
        "\n",
        "\n",
        "class ConvolutionModel(LanguageModel):\n",
        "    def build(self):\n",
        "        assert self.config['question_len'] == self.config['answer_len']\n",
        "\n",
        "        question = self.question\n",
        "        answer = self.get_answer()\n",
        "\n",
        "        # add embedding layers\n",
        "        weights = np.load(self.config['initial_embed_weights'])\n",
        "        embedding = Embedding(input_dim=self.config['n_words'],\n",
        "                              output_dim=weights.shape[1],\n",
        "                              weights=[weights])\n",
        "        question_embedding = embedding(question)\n",
        "        answer_embedding = embedding(answer)\n",
        "\n",
        "        hidden_layer = TimeDistributed(Dense(200, activation='tanh'))\n",
        "\n",
        "        question_hl = hidden_layer(question_embedding)\n",
        "        answer_hl = hidden_layer(answer_embedding)\n",
        "\n",
        "        # cnn\n",
        "        cnns = [Conv1D(kernel_size=kernel_size,\n",
        "                       filters=1000,\n",
        "                       activation='tanh',\n",
        "                       padding='same') for kernel_size in [2, 3, 5, 7]]\n",
        "        # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')\n",
        "        question_cnn = concatenate([cnn(question_hl) for cnn in cnns], axis=-1)\n",
        "        # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')\n",
        "        answer_cnn = concatenate([cnn(answer_hl) for cnn in cnns], axis=-1)\n",
        "\n",
        "        # maxpooling\n",
        "        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "        maxpool.supports_masking = True\n",
        "        # enc = Dense(100, activation='tanh')\n",
        "        # question_pool = enc(maxpool(question_cnn))\n",
        "        # answer_pool = enc(maxpool(answer_cnn))\n",
        "        question_pool = maxpool(question_cnn)\n",
        "        answer_pool = maxpool(answer_cnn)\n",
        "\n",
        "        return question_pool, answer_pool\n",
        "\n",
        "\n",
        "class ConvolutionalLSTM(LanguageModel):\n",
        "    def build(self):\n",
        "        question = self.question\n",
        "        answer = self.get_answer()\n",
        "\n",
        "        # add embedding layers\n",
        "        weights = np.load(self.config['initial_embed_weights'])\n",
        "        embedding = Embedding(input_dim=self.config['n_words'],\n",
        "                              output_dim=weights.shape[1],\n",
        "                              weights=[weights])\n",
        "        question_embedding = embedding(question)\n",
        "        answer_embedding = embedding(answer)\n",
        "\n",
        "        f_rnn = LSTM(141, return_sequences=True, implementation=1)\n",
        "        b_rnn = LSTM(141, return_sequences=True, implementation=1, go_backwards=True)\n",
        "\n",
        "        qf_rnn = f_rnn(question_embedding)\n",
        "        qb_rnn = b_rnn(question_embedding)\n",
        "        # question_pool = merge([qf_rnn, qb_rnn], mode='concat', concat_axis=-1)\n",
        "        question_pool = concatenate([qf_rnn, qb_rnn], axis=-1)\n",
        "\n",
        "        af_rnn = f_rnn(answer_embedding)\n",
        "        ab_rnn = b_rnn(answer_embedding)\n",
        "        # answer_pool = merge([af_rnn, ab_rnn], mode='concat', concat_axis=-1)\n",
        "        answer_pool = concatenate([af_rnn, ab_rnn], axis=-1)\n",
        "\n",
        "        # cnn\n",
        "        cnns = [Conv1D(kernel_size=kernel_size,\n",
        "                       filters=500,\n",
        "                       activation='tanh',\n",
        "                       padding='same') for kernel_size in [1, 2, 3, 5]]\n",
        "        # question_cnn = merge([cnn(question_pool) for cnn in cnns], mode='concat')\n",
        "        question_cnn = concatenate([cnn(question_pool) for cnn in cnns], axis=-1)\n",
        "        # answer_cnn = merge([cnn(answer_pool) for cnn in cnns], mode='concat')\n",
        "        answer_cnn = concatenate([cnn(answer_pool) for cnn in cnns], axis=-1)\n",
        "\n",
        "        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "        maxpool.supports_masking = True\n",
        "        question_pool = maxpool(question_cnn)\n",
        "        answer_pool = maxpool(answer_cnn)\n",
        "\n",
        "        return question_pool, answer_pool\n",
        "\n",
        "\n",
        "class AttentionModel(LanguageModel):\n",
        "    def build(self):\n",
        "        question = self.question\n",
        "        answer = self.get_answer()\n",
        "\n",
        "        # add embedding layers\n",
        "        weights = np.load(self.config['initial_embed_weights'])\n",
        "        embedding = Embedding(input_dim=self.config['n_words'],\n",
        "                              output_dim=weights.shape[1],\n",
        "                              # mask_zero=True,\n",
        "                              weights=[weights])\n",
        "        question_embedding = embedding(question)\n",
        "        answer_embedding = embedding(answer)\n",
        "\n",
        "        # question rnn part\n",
        "        f_rnn = LSTM(141, return_sequences=True, consume_less='mem')\n",
        "        b_rnn = LSTM(141, return_sequences=True, consume_less='mem', go_backwards=True)\n",
        "        question_f_rnn = f_rnn(question_embedding)\n",
        "        question_b_rnn = b_rnn(question_embedding)\n",
        "\n",
        "        # maxpooling\n",
        "        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "        maxpool.supports_masking = True\n",
        "        question_pool = merge([maxpool(question_f_rnn), maxpool(question_b_rnn)], mode='concat', concat_axis=-1)\n",
        "\n",
        "        # answer rnn part\n",
        "        from attention_lstm import AttentionLSTMWrapper\n",
        "        f_rnn = AttentionLSTMWrapper(f_rnn, question_pool, single_attention_param=True)\n",
        "        b_rnn = AttentionLSTMWrapper(b_rnn, question_pool, single_attention_param=True)\n",
        "\n",
        "        answer_f_rnn = f_rnn(answer_embedding)\n",
        "        answer_b_rnn = b_rnn(answer_embedding)\n",
        "        answer_pool = merge([maxpool(answer_f_rnn), maxpool(answer_b_rnn)], mode='concat', concat_axis=-1)\n",
        "\n",
        "        return question_pool, answer_pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cgVaKnz2Z9O-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Stack over flow evaluation"
      ]
    },
    {
      "metadata": {
        "id": "r34pn9zOaAcK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "import sys\n",
        "import random\n",
        "from time import strftime, gmtime, time\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import backend as K\n",
        "\n",
        "import threading\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def clear_session():\n",
        "    K.clear_session()\n",
        "\n",
        "def log(x):\n",
        "    print(x)\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, conf, model, optimizer=None):\n",
        "        try:\n",
        "            data_path = os.environ['STACK_OVER_FLOW_QA']\n",
        "        except KeyError:\n",
        "            print(\"STACK_OVER_FLOW_QA is not set. Set it to your clone of https://github.com/mrezende/stack_over_flow_python\")\n",
        "            sys.exit(1)\n",
        "        if isinstance(conf, str):\n",
        "            conf = json.load(open(conf, 'rb'))\n",
        "        self.model = model(conf)\n",
        "        self.path = data_path\n",
        "        self.conf = conf\n",
        "        self.params = conf['training']\n",
        "        optimizer = self.params['optimizer'] if optimizer is None else optimizer\n",
        "        self.model.compile(optimizer)\n",
        "\n",
        "        self.answers = self.load('answers.json') # self.load('generated')\n",
        "        self._vocab = None\n",
        "        self._reverse_vocab = None\n",
        "        self._eval_sets = None\n",
        "\n",
        "    ##### Resources #####\n",
        "\n",
        "    def load(self, name):\n",
        "        return json.load(open(os.path.join(self.path, name), 'r'))\n",
        "\n",
        "    def vocab(self):\n",
        "        if self._vocab is None:\n",
        "            reverse_vocab = self.reverse_vocab()\n",
        "            self._vocab = dict((v, k.lower()) for k, v in reverse_vocab.items())\n",
        "        return self._vocab\n",
        "\n",
        "    def reverse_vocab(self):\n",
        "        if self._reverse_vocab is None:\n",
        "            samples = self.load('samples_for_tokenizer.json')\n",
        "\n",
        "            tokenizer = Tokenizer()\n",
        "            tokenizer.fit_on_texts(samples)\n",
        "\n",
        "            self._reverse_vocab = tokenizer.word_index\n",
        "        return self._reverse_vocab\n",
        "\n",
        "    ##### Loading / saving #####\n",
        "\n",
        "    def save_epoch(self, epoch):\n",
        "        if not os.path.exists('models/'):\n",
        "            os.makedirs('models/')\n",
        "        self.model.save_weights('models/weights_epoch_%d.h5' % epoch, overwrite=True)\n",
        "\n",
        "    def load_epoch(self, epoch):\n",
        "        assert os.path.exists('models/weights_epoch_%d.h5' % epoch), 'Weights at epoch %d not found' % epoch\n",
        "        self.model.load_weights('models/weights_epoch_%d.h5' % epoch)\n",
        "\n",
        "    ##### Converting / reverting #####\n",
        "\n",
        "    def convert(self, words):\n",
        "        rvocab = self.reverse_vocab()\n",
        "        if type(words) == str:\n",
        "            words = words.strip().lower().split(' ')\n",
        "        return [rvocab.get(w, 0) for w in words]\n",
        "\n",
        "    def revert(self, indices):\n",
        "        vocab = self.vocab()\n",
        "        return [vocab.get(i, 'X') for i in indices]\n",
        "\n",
        "    ##### Padding #####\n",
        "\n",
        "    def padq(self, data):\n",
        "        return self.pad(data, self.conf.get('question_len', None))\n",
        "\n",
        "    def pada(self, data):\n",
        "        return self.pad(data, self.conf.get('answer_len', None))\n",
        "\n",
        "    def pad(self, data, len=None):\n",
        "        from keras.preprocessing.sequence import pad_sequences\n",
        "        return pad_sequences(data, maxlen=len, padding='post', truncating='post', value=0)\n",
        "\n",
        "    ##### Training #####\n",
        "\n",
        "    def get_time(self):\n",
        "        return strftime('%Y-%m-%d %H:%M:%S', gmtime())\n",
        "\n",
        "    def train(self):\n",
        "        batch_size = self.params['batch_size']\n",
        "        nb_epoch = self.params['nb_epoch']\n",
        "        validation_split = self.params['validation_split']\n",
        "\n",
        "        training_set = self.load('train.json')\n",
        "        # top_50 = self.load('top_50')\n",
        "\n",
        "        questions = list()\n",
        "        good_answers = list()\n",
        "        indices = list()\n",
        "\n",
        "        for j, q in enumerate(training_set):\n",
        "            questions += [q['question']] * len(q['answers'])\n",
        "            good_answers += [i for i in q['answers']]\n",
        "            indices += [j] * len(q['answers'])\n",
        "        log('Began training at %s on %d samples' % (self.get_time(), len(questions)))\n",
        "\n",
        "        questions = self.padq(questions)\n",
        "        good_answers = self.pada(good_answers)\n",
        "\n",
        "        val_loss = {'loss': 1., 'epoch': 0}\n",
        "\n",
        "        # def get_bad_samples(indices, top_50):\n",
        "        #     return [self.answers[random.choice(top_50[i])] for i in indices]\n",
        "\n",
        "        for i in range(1, nb_epoch+1):\n",
        "            # sample from all answers to get bad answers\n",
        "            # if i % 2 == 0:\n",
        "            #     bad_answers = self.pada(random.sample(self.answers.values(), len(good_answers)))\n",
        "            # else:\n",
        "            #     bad_answers = self.pada(get_bad_samples(indices, top_50))\n",
        "            bad_answers = self.pada(random.sample(self.answers, len(good_answers)))\n",
        "\n",
        "            print('Fitting epoch %d' % i, file=sys.stderr)\n",
        "            hist = self.model.fit([questions, good_answers, bad_answers], epochs=1, batch_size=batch_size,\n",
        "                                  validation_split=validation_split, verbose=2)\n",
        "\n",
        "            if hist.history['val_loss'][0] < val_loss['loss']:\n",
        "                val_loss = {'loss': hist.history['val_loss'][0], 'epoch': i}\n",
        "            log('%s -- Epoch %d ' % (self.get_time(), i) +\n",
        "                'Loss = %.4f, Validation Loss = %.4f ' % (hist.history['loss'][0], hist.history['val_loss'][0]) +\n",
        "                '(Best: Loss = %.4f, Epoch = %d)' % (val_loss['loss'], val_loss['epoch']))\n",
        "\n",
        "            self.save_epoch(i)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    ##### Evaluation #####\n",
        "\n",
        "    def prog_bar(self, so_far, total, n_bars=20):\n",
        "        n_complete = int(so_far * n_bars / total)\n",
        "        if n_complete >= n_bars - 1:\n",
        "            print('\\r[' + '=' * n_bars + ']', end='', file=sys.stderr)\n",
        "        else:\n",
        "            s = '\\r[' + '=' * (n_complete - 1) + '>' + '.' * (n_bars - n_complete) + ']'\n",
        "            print(s, end='', file=sys.stderr)\n",
        "\n",
        "    def eval_sets(self):\n",
        "        if self._eval_sets is None:\n",
        "            self._eval_sets = dict([(s, self.load(s)) for s in ['test.json']])\n",
        "        return self._eval_sets\n",
        "\n",
        "    def get_score(self, verbose=False):\n",
        "        top1_ls = []\n",
        "        mrr_ls = []\n",
        "        for name, data in self.eval_sets().items():\n",
        "            print('----- %s -----' % name)\n",
        "\n",
        "            random.shuffle(data)\n",
        "\n",
        "            if 'n_eval' in self.params:\n",
        "                data = data[:self.params['n_eval']]\n",
        "\n",
        "            c_1, c_2 = 0, 0\n",
        "\n",
        "            for i, d in enumerate(data):\n",
        "                self.prog_bar(i, len(data))\n",
        "\n",
        "                answers = d['good'] + d['bad']\n",
        "                answers = self.pada(answers)\n",
        "                question = self.padq([d['question']] * len(answers))\n",
        "\n",
        "                sims = self.model.predict([question, answers])\n",
        "\n",
        "                n_good = len(d['good'])\n",
        "                max_r = np.argmax(sims)\n",
        "                max_n = np.argmax(sims[:n_good])\n",
        "\n",
        "                r = rankdata(sims, method='max')\n",
        "\n",
        "                #if verbose:\n",
        "                #    min_r = np.argmin(sims)\n",
        "                #    amin_r = self.answers[indices[min_r]]\n",
        "                #     amax_r = self.answers[indices[max_r]]\n",
        "                #     amax_n = self.answers[indices[max_n]]\n",
        "                #\n",
        "                #     print(' '.join(self.revert(d['question'])))\n",
        "                #     print('Predicted: ({}) '.format(sims[max_r]) + ' '.join(self.revert(amax_r)))\n",
        "                #     print('Expected: ({}) Rank = {} '.format(sims[max_n], r[max_n]) + ' '.join(self.revert(amax_n)))\n",
        "                #     print('Worst: ({})'.format(sims[min_r]) + ' '.join(self.revert(amin_r)))\n",
        "\n",
        "                c_1 += 1 if max_r == max_n else 0\n",
        "                c_2 += 1 / float(r[max_r] - r[max_n] + 1)\n",
        "\n",
        "            top1 = c_1 / float(len(data))\n",
        "            mrr = c_2 / float(len(data))\n",
        "\n",
        "            del data\n",
        "            print('Top-1 Precision: %f' % top1)\n",
        "            print('MRR: %f' % mrr)\n",
        "            top1_ls.append(top1)\n",
        "            mrr_ls.append(mrr)\n",
        "        return top1_ls, mrr_ls\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) >= 2 and sys.argv[1] == 'serve':\n",
        "        from flask import Flask\n",
        "        app = Flask(__name__)\n",
        "        port = 5000\n",
        "        lines = list()\n",
        "        def log(x):\n",
        "            lines.append(x)\n",
        "\n",
        "        @app.route('/')\n",
        "        def home():\n",
        "            return ('<html><body><h1>Training Log</h1>' +\n",
        "                    ''.join(['<code>{}</code><br/>'.format(line) for line in lines]) +\n",
        "                    '</body></html>')\n",
        "\n",
        "        def start_server():\n",
        "            app.run(debug=False, use_evalex=False, port=port)\n",
        "\n",
        "        threading.Thread(target=start_server, args=tuple()).start()\n",
        "        print('Serving to port %d' % port, file=sys.stderr)\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    confs = json.load(open('stack_over_flow_conf.json', 'r'))\n",
        "\n",
        "    from keras_models import EmbeddingModel, ConvolutionModel, ConvolutionalLSTM\n",
        "    for conf in confs:\n",
        "        print(conf)\n",
        "        evaluator = Evaluator(conf, model=ConvolutionalLSTM, optimizer='adam')\n",
        "\n",
        "        # train the model\n",
        "        best_loss = evaluator.train()\n",
        "\n",
        "        # evaluate mrr for a particular epoch\n",
        "        evaluator.load_epoch(best_loss['epoch'])\n",
        "        top1, mrr = evaluator.get_score(verbose=False)\n",
        "        log(' - Top-1 Precision:')\n",
        "        log('   - %.3f on test 1' % top1[0])\n",
        "\n",
        "        log(' - MRR:')\n",
        "        log('   - %.3f on test 1' % mrr[0])\n",
        "        clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}